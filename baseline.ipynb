{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/netapp2/home/cs1839/miniforge3/envs/capstone/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /PHShome/cs1839/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import json\n",
    "from huggingface_hub import login\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import re\n",
    "\n",
    "\n",
    "# Read the JSON config file\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Get the token from the JSON file\n",
    "hg_token = config['HuggingFace']['token']\n",
    "# Login using the token\n",
    "login(token=hg_token)\n",
    "\n",
    "# LLM folder\n",
    "llm_folder = \"/PHShome/jn180/llm_public_host\"\n",
    "# Data folder\n",
    "data_folder = \"/PHShome/cs1839/capstone_data/\"\n",
    "# results table path\n",
    "results_df_path = data_folder + \"results.csv\"\n",
    "\n",
    "# data to inference \n",
    "medication_status_test = pd.read_csv(data_folder + \"medication_status_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/netapp3/raw_data3/share/llm_public_host/Llama-3.1-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos_token (common for autoregressive models)\n",
    "tokenizer.padding_side = \"left\"  # Set padding to left for autoregressive models\n",
    "\n",
    "# Initialize the pipeline for text generation\n",
    "generator = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model_path,\n",
    "    tokenizer=tokenizer,  # Pass the tokenizer with left padding settings\n",
    "    device=0,  # '0' for GPU, '-1' for CPU\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16} # Use torch.bfloat16 for faster generation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:15<00:00, 19.30s/it]\n"
     ]
    }
   ],
   "source": [
    "sub_df = medication_status_test['snippet'].values.tolist()\n",
    "\n",
    "batch_size = 16\n",
    "num_step = len(sub_df) // batch_size + 1 if len(sub_df) % batch_size != 0 else len(sub_df) // batch_size\n",
    "max_token_output = 80\n",
    "response_list = []\n",
    "\n",
    "# Settings for text generation\n",
    "use_sampling = True  # Set to True if you want to use sampling; False for greedy search\n",
    "temperature = 0.1 if use_sampling else None  # Set temperature for sampling; None for greedy\n",
    "top_p = 0.9 if use_sampling else None  # Use top-p sampling only when sampling is enabled\n",
    "\n",
    "# Iterate through batches\n",
    "for i in tqdm(range(num_step)):\n",
    "    input_texts = sub_df[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "    prompt = \"\"\"\n",
    "Identify and categorize the medications mentioned in the following medical note. Extract all medications the patient has taken before, is currently taking, and any other medications mentioned.\n",
    "Note: Adjust the number of medications in each category based on the input. Write None if no other medication mentioned. Strictly follow the output format.\n",
    "Expected Output Format:\n",
    "\"\n",
    "- Current Medications (Active): [Medication 1], [Medication 2]\n",
    "- Discontinued Medications: [Medication 3], [Medication 4]\n",
    "- Other Mentioned Medications (neither active nor discontinued): [Medication 5], [Medication 6]\n",
    "END\"\n",
    "\n",
    "Input Medical Note:\n",
    "\"\"\"\n",
    "    output = \"\"\"\n",
    "    \\nOutput:\\n\n",
    "    \"\"\"\n",
    "\n",
    "    input_texts = [prompt + text + output for text in input_texts]\n",
    "\n",
    "    # Generate responses for each batch\n",
    "    responses = generator(\n",
    "        input_texts,  # Concatenate the prompt and input texts\n",
    "        max_new_tokens=max_token_output,   # Limit the number of new tokens in the output\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,  # Set the pad_token_id\n",
    "        eos_token_id=generator.tokenizer.eos_token_id,  # Set the eos_token_id\n",
    "        \n",
    "        truncation=True,          # Truncate the input if it's longer than max_token_input\n",
    "        do_sample=use_sampling,   # Sampling or greedy search\n",
    "        temperature=temperature,  # Only set if sampling is enabled\n",
    "        top_p=top_p,              # Only set if sampling is enabled\n",
    "    )\n",
    "\n",
    "    # Loop through each input and its corresponding response\n",
    "    for response in responses:\n",
    "        # Each `response` is a list with one dictionary, so we need to extract the first item\n",
    "        for generated in response:  # Loop through the list in case of multiple generations\n",
    "            # only save the generated output\n",
    "            response_list.append(generated['generated_text'].split(\"\\nOutput:\\n\")[1].split(\"END\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1495997/3797027712.py:152: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_df._append(new_row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MIT</td>\n",
       "      <td>meta-llama/Meta-Llama-3.1-8B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.602556</td>\n",
       "      <td>0.688087</td>\n",
       "      <td>0.745139</td>\n",
       "      <td>0.629691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset                                  Model  \\\n",
       "0     MIT  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
       "\n",
       "                                              Prompt  Accuracy  Precision  \\\n",
       "0  \\nIdentify and categorize the medications ment...  0.602556   0.688087   \n",
       "\n",
       "     Recall        F1  \n",
       "0  0.745139  0.629691  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_output(input_df, response_list):\n",
    "    \"\"\"\n",
    "    Processes a list of LLM responses to extract medication information and adds it to the input DataFrame.\n",
    "\n",
    "    This function takes an input DataFrame (`input_df`) and a list of responses (`response_list`),\n",
    "    where each response contains categorized medication data. The function extracts three categories\n",
    "    of medications (active, discontinued, and neither), formats them into lists, and creates a new\n",
    "    DataFrame with three columns:\n",
    "    \n",
    "    - `active_medications`: Medications that the patient is currently taking.\n",
    "    - `discontinued_medications`: Medications that the patient has taken but has since discontinued.\n",
    "    - `neither_medications`: Medications that are mentioned but are neither currently taken nor discontinued.\n",
    "\n",
    "    The new DataFrame with these three columns is concatenated with the `input_df` and returned.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_df : pd.DataFrame\n",
    "        The original input DataFrame, which will be concatenated with the extracted medication data.\n",
    "    \n",
    "    response_list : list of str\n",
    "        A list of strings containing the LLM responses. Each response includes a categorized list of medications\n",
    "        (active, discontinued, and neither).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame that concatenates the `input_df` with the extracted medication data.\n",
    "        The resulting DataFrame will have the original columns from `input_df`, plus three new columns:\n",
    "        `active_medications`, `discontinued_medications`, and `neither_medications`, each containing a list of medications.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> input_df = pd.DataFrame({'notes': [\"Note 1\", \"Note 2\"]})\n",
    "    >>> response_list = [\n",
    "    >>>     'Current Medications (Active): Aspirin\\nDiscontinued Medications: Atenolol\\nOther Mentioned Medications: Ibuprofen',\n",
    "    >>>     'Current Medications (Active): None\\nDiscontinued Medications: Metoprolol\\nOther Mentioned Medications: Acetaminophen'\n",
    "    >>> ]\n",
    "    >>> final_df = process_output(input_df, response_list)\n",
    "    >>> print(final_df)\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "        notes    active_medications     discontinued_medications    neither_medications\n",
    "        Note 1   [Aspirin]              [Atenolol]                  [Ibuprofen]\n",
    "        Note 2   []                     [Metoprolol]                [Acetaminophen]\n",
    "    \"\"\"\n",
    "    # Initialize lists to store the medications for each category\n",
    "    active_medications_list = []\n",
    "    discontinued_medications_list = []\n",
    "    neither_medications_list = []\n",
    "\n",
    "    # Loop through each response in the response_list\n",
    "    for response in response_list:\n",
    "        # Extract the active, discontinued, and neither medications using regular expressions\n",
    "        active_medications = re.search(r'Current Medications \\(Active\\):\\s*(.*)', response)\n",
    "        discontinued_medications = re.search(r'Discontinued Medications:\\s*(.*)', response)\n",
    "        neither_medications = re.search(r'Other Mentioned Medications.*:\\s*(.*)', response)\n",
    "\n",
    "        # Convert to lists and handle None cases\n",
    "        active_medications = active_medications.group(1).split(', ') if active_medications and active_medications.group(1) != \"None\" else []\n",
    "        discontinued_medications = discontinued_medications.group(1).split(', ') if discontinued_medications and discontinued_medications.group(1) != \"None\" else []\n",
    "        neither_medications = neither_medications.group(1).split(', ') if neither_medications and neither_medications.group(1) != \"None\" else []\n",
    "\n",
    "        # Append each category list to their respective main lists\n",
    "        active_medications_list.append(active_medications)\n",
    "        discontinued_medications_list.append(discontinued_medications)\n",
    "        neither_medications_list.append(neither_medications)\n",
    "\n",
    "    # Create a new DataFrame from the lists\n",
    "    output_df = pd.DataFrame({\n",
    "        'active_medications_pred': active_medications_list,\n",
    "        'discontinued_medications_pred': discontinued_medications_list,\n",
    "        'neither_medications_pred': neither_medications_list\n",
    "    })\n",
    "\n",
    "    # Concatenate the input_df with the output_df by rows\n",
    "    result_df = pd.concat([input_df, output_df], axis=1)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def calculate_row_metrics(df):\n",
    "    columns = df.columns.tolist()\n",
    "    # Iterate over the three categories\n",
    "    for category in ['active_medications', 'discontinued_medications', 'neither_medications']:\n",
    "        true_col = category\n",
    "        pred_col = category + '_pred'\n",
    "\n",
    "        # Check the type of true_col, if not list, use eval to convert back\n",
    "        if not isinstance(df[true_col][0], list):\n",
    "            df[true_col] = df[true_col].apply(lambda x: eval(x))\n",
    "\n",
    "        # Initialize columns to store row-wise metrics\n",
    "        df.loc[:, 'avg_precision'] = np.nan\n",
    "        df.loc[:, 'avg_recall'] = np.nan\n",
    "        df.loc[:, 'avg_f1'] = np.nan\n",
    "        df.loc[:, 'avg_accuracy'] = np.nan\n",
    "\n",
    "        # For each row, compute metrics\n",
    "        for index, row in df.iterrows():\n",
    "            # Convert lists to sets for easier comparison\n",
    "            true_set = set(row[true_col])\n",
    "            pred_set = set(row[pred_col])\n",
    "            \n",
    "            # Check if both sets are empty\n",
    "            if not true_set and not pred_set:\n",
    "                precision, recall, f1, accuracy = 1.0, 1.0, 1.0, 1.0  # perfect scores when both are empty\n",
    "            else:\n",
    "                # Create binary lists: 1 if medication is present, 0 otherwise\n",
    "                all_medications = list(true_set.union(pred_set))\n",
    "                true_binary = [1 if med in true_set else 0 for med in all_medications]\n",
    "                pred_binary = [1 if med in pred_set else 0 for med in all_medications]\n",
    "\n",
    "                # Calculate precision, recall, F1, accuracy\n",
    "                precision = precision_score(true_binary, pred_binary, zero_division=1)\n",
    "                recall = recall_score(true_binary, pred_binary, zero_division=1)\n",
    "                f1 = f1_score(true_binary, pred_binary, zero_division=1)\n",
    "                accuracy = accuracy_score(true_binary, pred_binary)\n",
    "\n",
    "            # Append the metrics to the DataFrame\n",
    "            df.loc[index, f'{category}_precision'] = precision\n",
    "            df.loc[index, f'{category}_recall'] = recall\n",
    "            df.loc[index, f'{category}_f1'] = f1\n",
    "            df.loc[index, f'{category}_accuracy'] = accuracy\n",
    "        \n",
    "    # get the average of each metric and append to a column as avg_precision, avg_recall, avg_f1, avg_accuracy\n",
    "    df['avg_precision'] = df[['active_medications_precision', 'discontinued_medications_precision', 'neither_medications_precision']].mean(axis=1)\n",
    "    df['avg_recall'] = df[['active_medications_recall', 'discontinued_medications_recall', 'neither_medications_recall']].mean(axis=1)\n",
    "    df['avg_f1'] = df[['active_medications_f1', 'discontinued_medications_f1', 'neither_medications_f1']].mean(axis=1)\n",
    "    df['avg_accuracy'] = df[['active_medications_accuracy', 'discontinued_medications_accuracy', 'neither_medications_accuracy']].mean(axis=1)\n",
    "\n",
    "    return df[columns+['avg_precision', 'avg_recall', 'avg_f1', 'avg_accuracy']]\n",
    "\n",
    "\n",
    "df_w_classifications = process_output(medication_status_test, response_list)\n",
    "df_w_row_metrics = calculate_row_metrics(df_w_classifications)\n",
    "\n",
    "result_df = pd.read_csv(data_folder+'results.csv')\n",
    "metrics_mean = df_w_row_metrics[['avg_precision', 'avg_recall', 'avg_f1', 'avg_accuracy']].mean(axis=0)\n",
    "\n",
    "# Define your result row\n",
    "new_row = {\n",
    "    'Dataset': 'MIT',\n",
    "    'Model': model_path.split('/')[-1],\n",
    "    'Prompt': prompt,\n",
    "    'Accuracy': metrics_mean.get('avg_accuracy', np.nan),\n",
    "    'Precision': metrics_mean.get('avg_precision', np.nan),\n",
    "    'Recall': metrics_mean.get('avg_recall', np.nan),\n",
    "    'F1': metrics_mean.get('avg_f1', np.nan)\n",
    "}\n",
    "\n",
    "result_df._append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# 1. Function to initialize model and tokenizer\n",
    "def initialize_model(model_path, device=0, use_fp16=True):\n",
    "    \"\"\"\n",
    "    Initializes the model and tokenizer for text generation.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    model_path : str\n",
    "        Path of the model to be loaded.\n",
    "    device : int\n",
    "        Device to use, 0 for GPU and -1 for CPU.\n",
    "    use_fp16 : bool\n",
    "        Whether to use FP16 for inference.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    generator : pipeline\n",
    "        A HuggingFace pipeline ready for text generation.\n",
    "    \"\"\"\n",
    "    # Load tokenizer and set padding side to left\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos_token (common for autoregressive models)\n",
    "    tokenizer.padding_side = \"left\"  # Set padding to left for autoregressive models\n",
    "\n",
    "    # Initialize the pipeline for text generation\n",
    "    generator = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model_path,\n",
    "        tokenizer=tokenizer,  # Pass the tokenizer with left padding settings\n",
    "        device=device,  # '0' for GPU, '-1' for CPU\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16} if use_fp16 else {}\n",
    "    )\n",
    "    return generator\n",
    "\n",
    "# 2. Function to generate batch responses using the model\n",
    "    return response_list\n",
    "\n",
    "# 2. Function to generate batch responses using the model\n",
    "def generate_responses(input_df, batch_size, generator, prompt_template, max_token_output=80, use_sampling=True):\n",
    "    \"\"\"\n",
    "    Generate text responses in batches using the generator.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_df : list of str\n",
    "        List of input texts to run inference on.\n",
    "    batch_size : int\n",
    "        Size of each batch for inference.\n",
    "    generator : pipeline\n",
    "        HuggingFace pipeline initialized for text generation.\n",
    "    prompt_template : str\n",
    "        The template for the prompt to be used.\n",
    "    max_token_output : int\n",
    "        Maximum number of tokens to generate.\n",
    "    use_sampling : bool\n",
    "        Whether to use sampling or greedy decoding.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    response_list : list of str\n",
    "        List of generated responses.\n",
    "    \"\"\"\n",
    "    sub_df = input_df['snippet'].values.tolist()\n",
    "\n",
    "    response_list = []\n",
    "    num_step = len(sub_df) // batch_size + (1 if len(sub_df) % batch_size != 0 else 0)\n",
    "    temperature = 0.1 if use_sampling else None\n",
    "    top_p = 0.9 if use_sampling else None\n",
    "\n",
    "    for i in tqdm(range(num_step)):\n",
    "        input_texts = sub_df[i*batch_size:(i+1)*batch_size]\n",
    "        input_texts = [prompt_template.format(text) for text in input_texts]\n",
    "\n",
    "        # Generate the responses\n",
    "        responses = generator(\n",
    "            input_texts,  \n",
    "            max_new_tokens=max_token_output,  \n",
    "            pad_token_id=generator.tokenizer.eos_token_id,\n",
    "            eos_token_id=generator.tokenizer.eos_token_id,\n",
    "            truncation=True,\n",
    "            do_sample=use_sampling,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "\n",
    "        # Process the output\n",
    "        for response in responses:\n",
    "            for generated in response:\n",
    "                # Extract relevant part of the response and append to list\n",
    "                response_list.append(generated['generated_text'].split(\"\\nOutput:\\n\")[1].split(\"END\")[0])\n",
    "    \n",
    "    return response_list\n",
    "\n",
    "# 3. Function to process the LLM output\n",
    "def process_output(input_df, response_list):\n",
    "    \"\"\"\n",
    "    Processes a list of LLM responses to extract medication information and adds it to the input DataFrame.\n",
    "\n",
    "    This function takes an input DataFrame (`input_df`) and a list of responses (`response_list`),\n",
    "    where each response contains categorized medication data. The function extracts three categories\n",
    "    of medications (active, discontinued, and neither), formats them into lists, and creates a new\n",
    "    DataFrame with three columns:\n",
    "    \n",
    "    - `active_medications`: Medications that the patient is currently taking.\n",
    "    - `discontinued_medications`: Medications that the patient has taken but has since discontinued.\n",
    "    - `neither_medications`: Medications that are mentioned but are neither currently taken nor discontinued.\n",
    "\n",
    "    The new DataFrame with these three columns is concatenated with the `input_df` and returned.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_df : pd.DataFrame\n",
    "        The original input DataFrame, which will be concatenated with the extracted medication data.\n",
    "    \n",
    "    response_list : list of str\n",
    "        A list of strings containing the LLM responses. Each response includes a categorized list of medications\n",
    "        (active, discontinued, and neither).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame that concatenates the `input_df` with the extracted medication data.\n",
    "        The resulting DataFrame will have the original columns from `input_df`, plus three new columns:\n",
    "        `active_medications`, `discontinued_medications`, and `neither_medications`, each containing a list of medications.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> input_df = pd.DataFrame({'notes': [\"Note 1\", \"Note 2\"]})\n",
    "    >>> response_list = [\n",
    "    >>>     'Current Medications (Active): Aspirin\\nDiscontinued Medications: Atenolol\\nOther Mentioned Medications: Ibuprofen',\n",
    "    >>>     'Current Medications (Active): None\\nDiscontinued Medications: Metoprolol\\nOther Mentioned Medications: Acetaminophen'\n",
    "    >>> ]\n",
    "    >>> final_df = process_output(input_df, response_list)\n",
    "    >>> print(final_df)\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "        notes    active_medications     discontinued_medications    neither_medications\n",
    "        Note 1   [Aspirin]              [Atenolol]                  [Ibuprofen]\n",
    "        Note 2   []                     [Metoprolol]                [Acetaminophen]\n",
    "    \"\"\"\n",
    "    # Initialize lists to store the medications for each category\n",
    "    active_medications_list = []\n",
    "    discontinued_medications_list = []\n",
    "    neither_medications_list = []\n",
    "\n",
    "    # Loop through each response in the response_list\n",
    "    for response in response_list:\n",
    "        # Extract the active, discontinued, and neither medications using regular expressions\n",
    "        active_medications = re.search(r'Current Medications \\(Active\\):\\s*(.*)', response)\n",
    "        discontinued_medications = re.search(r'Discontinued Medications:\\s*(.*)', response)\n",
    "        neither_medications = re.search(r'Other Mentioned Medications.*:\\s*(.*)', response)\n",
    "\n",
    "        # Convert to lists and handle None cases\n",
    "        active_medications = active_medications.group(1).split(', ') if active_medications and active_medications.group(1) != \"None\" else []\n",
    "        discontinued_medications = discontinued_medications.group(1).split(', ') if discontinued_medications and discontinued_medications.group(1) != \"None\" else []\n",
    "        neither_medications = neither_medications.group(1).split(', ') if neither_medications and neither_medications.group(1) != \"None\" else []\n",
    "\n",
    "        # Append each category list to their respective main lists\n",
    "        active_medications_list.append(active_medications)\n",
    "        discontinued_medications_list.append(discontinued_medications)\n",
    "        neither_medications_list.append(neither_medications)\n",
    "\n",
    "    # Create a new DataFrame from the lists\n",
    "    output_df = pd.DataFrame({\n",
    "        'active_medications_pred': active_medications_list,\n",
    "        'discontinued_medications_pred': discontinued_medications_list,\n",
    "        'neither_medications_pred': neither_medications_list\n",
    "    })\n",
    "\n",
    "    # Concatenate the input_df with the output_df by rows\n",
    "    result_df = pd.concat([input_df, output_df], axis=1)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# 4. Function to calculate metrics (Precision, Recall, F1, Accuracy)\n",
    "def calculate_row_metrics(df):\n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    # Iterate over the three categories\n",
    "    for category in ['active_medications', 'discontinued_medications', 'neither_medications']:\n",
    "        true_col = category\n",
    "        pred_col = category + '_pred'\n",
    "\n",
    "        # Check the type of true_col, if not list, use eval to convert back\n",
    "        # Ensure that the column contains lists (if it's a string, evaluate it)\n",
    "        df[true_col] = df[true_col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "        # Initialize columns to store row-wise metrics\n",
    "        df.loc[:, 'avg_precision'] = np.nan\n",
    "        df.loc[:, 'avg_recall'] = np.nan\n",
    "        df.loc[:, 'avg_f1'] = np.nan\n",
    "        df.loc[:, 'avg_accuracy'] = np.nan\n",
    "\n",
    "        # For each row, compute metrics\n",
    "        for index, row in df.iterrows():\n",
    "            # Convert lists to sets for easier comparison\n",
    "            true_set = set(row[true_col])\n",
    "            pred_set = set(row[pred_col])\n",
    "            \n",
    "            # Check if both sets are empty\n",
    "            if not true_set and not pred_set:\n",
    "                precision, recall, f1, accuracy = 1.0, 1.0, 1.0, 1.0  # perfect scores when both are empty\n",
    "            else:\n",
    "                # Create binary lists: 1 if medication is present, 0 otherwise\n",
    "                all_medications = list(true_set.union(pred_set))\n",
    "                true_binary = [1 if med in true_set else 0 for med in all_medications]\n",
    "                pred_binary = [1 if med in pred_set else 0 for med in all_medications]\n",
    "\n",
    "                # Calculate precision, recall, F1, accuracy\n",
    "                precision = precision_score(true_binary, pred_binary, zero_division=1)\n",
    "                recall = recall_score(true_binary, pred_binary, zero_division=1)\n",
    "                f1 = f1_score(true_binary, pred_binary, zero_division=1)\n",
    "                accuracy = accuracy_score(true_binary, pred_binary)\n",
    "\n",
    "            # Append the metrics to the DataFrame\n",
    "            df.loc[index, f'{category}_precision'] = precision\n",
    "            df.loc[index, f'{category}_recall'] = recall\n",
    "            df.loc[index, f'{category}_f1'] = f1\n",
    "            df.loc[index, f'{category}_accuracy'] = accuracy\n",
    "        \n",
    "    # get the average of each metric and append to a column as avg_precision, avg_recall, avg_f1, avg_accuracy\n",
    "    df['avg_precision'] = df[['active_medications_precision', 'discontinued_medications_precision', 'neither_medications_precision']].mean(axis=1)\n",
    "    df['avg_recall'] = df[['active_medications_recall', 'discontinued_medications_recall', 'neither_medications_recall']].mean(axis=1)\n",
    "    df['avg_f1'] = df[['active_medications_f1', 'discontinued_medications_f1', 'neither_medications_f1']].mean(axis=1)\n",
    "    df['avg_accuracy'] = df[['active_medications_accuracy', 'discontinued_medications_accuracy', 'neither_medications_accuracy']].mean(axis=1)\n",
    "\n",
    "    return df[columns+['avg_precision', 'avg_recall', 'avg_f1', 'avg_accuracy']]\n",
    "\n",
    "# 5. Main function to tie everything together\n",
    "def run_pipeline(model_path, input_df, prompt_template, batch_size=16, max_token_output=80, use_sampling=True):\n",
    "    \"\"\"\n",
    "    Main function to run the text generation pipeline and compute metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    model_path : str\n",
    "        The path of the model to be used.\n",
    "    input_df : pd.DataFrame\n",
    "        The data to be inferred.\n",
    "    prompt_template : str\n",
    "        Template for constructing the prompts.\n",
    "    batch_size : int\n",
    "        Number of examples per batch.\n",
    "    max_token_output : int\n",
    "        Maximum number of tokens to generate.\n",
    "    use_sampling : bool\n",
    "        Whether to use sampling (or greedy decoding).\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    result_df : pd.DataFrame\n",
    "        DataFrame with the processed outputs and calculated metrics.\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    generator = initialize_model(model_path, device=0)\n",
    "\n",
    "    # Generate responses\n",
    "    response_list = generate_responses(input_df, batch_size, generator, prompt_template, max_token_output, use_sampling)\n",
    "\n",
    "    # Process the responses to categorize medications\n",
    "    df_w_classifications = process_output(input_df, response_list)\n",
    "\n",
    "    # Calculate row-level metrics\n",
    "    df_w_metrics = calculate_row_metrics(df_w_classifications)\n",
    "\n",
    "    # Return the final DataFrame with metrics\n",
    "    return df_w_metrics\n",
    "\n",
    "# 6. Function to benchmark the model\n",
    "def benchmark_model(name_dataset, model_path, prompt_template, input_df, data_folder, result_df_path, use_sampling=True, batch_size=16, max_token_output=80):\n",
    "    # Run the pipeline\n",
    "    df_w_row_metrics = run_pipeline(model_path=model_path, \n",
    "                                    input_df=input_df, \n",
    "                                    prompt_template=prompt_template, \n",
    "                                    use_sampling=use_sampling,\n",
    "                                    batch_size=batch_size, \n",
    "                                    max_token_output=max_token_output)\n",
    "\n",
    "    result_df = pd.read_csv(data_folder+'results.csv')\n",
    "    metrics_mean = df_w_row_metrics[['avg_precision', 'avg_recall', 'avg_f1', 'avg_accuracy']].mean(axis=0)\n",
    "\n",
    "    # Define your result row\n",
    "    new_row = {\n",
    "        'Dataset': name_dataset,\n",
    "        'Model': model_path.split('/')[-1],\n",
    "        'Prompt': prompt_template,\n",
    "        'Accuracy': metrics_mean.get('avg_accuracy', np.nan),\n",
    "        'Precision': metrics_mean.get('avg_precision', np.nan),\n",
    "        'Recall': metrics_mean.get('avg_recall', np.nan),\n",
    "        'F1': metrics_mean.get('avg_f1', np.nan)\n",
    "    }\n",
    "\n",
    "    result_df = result_df._append(new_row, ignore_index=True).round(3)\n",
    "    result_df.to_csv(result_df_path, index=False)\n",
    "\n",
    "\n",
    "def clear_cuda_memory():\n",
    "    # Clear the cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run garbage collection\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model_paths ={   \n",
    "    \"Bio_ClinicalBERT\": \"/PHShome/jn180/llm_public_host/Bio_ClinicalBERT\",\n",
    "\n",
    "    \"Llama-3.1-8B\": \"/netapp3/raw_data3/share/llm_public_host/Llama-3.1-8B\",\n",
    "    \"Llama-3.1-8B-Instruct\": \"/netapp3/raw_data3/share/llm_public_host/Llama-3.1-8B-Instruct\",\n",
    "\n",
    "    \"Llama-3.2-1B-Instruct\": \"/netapp3/raw_data3/share/llm_public_host/Llama-3.2-1B-Instruct\",\n",
    "    \"Llama-3.2-3B-Instruct\": \"/netapp3/raw_data3/share/llm_public_host/Llama-3.2-3B-Instruct\",\n",
    "\n",
    "    \"Qwen2-7B-Instruct\": \"/PHShome/jn180/llm_public_host/Qwen2-7B-Instruct\",\n",
    "    \"Qwen2.5-14B-Instruct\": \"/netapp3/raw_data3/share/llm_public_host/Qwen2.5-14B-Instruct\",\n",
    "\n",
    "    \"meditron-7b\": \"/PHShome/jn180/llm_public_host/meditron-7b\",\n",
    "\n",
    "}\n",
    "\n",
    "name_dataset = \"MIT\"\n",
    "data_folder = \"/PHShome/cs1839/capstone_data/\"\n",
    "results_df_path = data_folder + \"results.csv\"\n",
    "medication_status_test = pd.read_csv(data_folder + \"medication_status_test.csv\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Identify and categorize the medications mentioned in the following medical note. Extract all medications the patient has taken before, is currently taking, and any other medications mentioned.\n",
    "Note: Adjust the number of medications in each category based on the input. Write None if no other medication mentioned. Strictly follow the output format.\n",
    "Expected Output Format:\n",
    "\"\n",
    "- Current Medications (Active): [Medication 1], [Medication 2]\n",
    "- Discontinued Medications: [Medication 3], [Medication 4]\n",
    "- Other Mentioned Medications (neither active nor discontinued): [Medication 5], [Medication 6]\n",
    "END\"\n",
    "\n",
    "Input Medical Note:\n",
    "{}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "for model_name, model_path in name_model_paths.items():\n",
    "    benchmark_model(name_dataset = name_dataset,\n",
    "                    model_path = model_path,\n",
    "                    prompt_template = prompt_template,\n",
    "                    input_df = medication_status_test,\n",
    "                    data_folder = data_folder,\n",
    "                    result_df_path = results_df_path,\n",
    "                    use_sampling = False,\n",
    "                    batch_size = 16,\n",
    "                    max_token_output = 80)\n",
    "\n",
    "\n",
    "# name_dataset = \"MIMIC-IV\"\n",
    "# mimic_iv = pd.read_csv(data_folder + \"mimic_iv_snippets.csv\")\n",
    "# # convert the active, discontinued, and neither medications to contained in a list\n",
    "# mimic_iv['active_medications'] = mimic_iv['active_medications'].apply(lambda x: [med.strip() for med in x.split(',')] if x is not np.nan else [])\n",
    "# mimic_iv['discontinued_medications'] = mimic_iv['discontinued_medications'].apply(lambda x: [med.strip() for med in x.split(',')] if x is not np.nan else [])\n",
    "# mimic_iv['neither_medications'] = mimic_iv['neither_medications'].apply(lambda x: [med.strip() for med in x.split(',')] if x is not np.nan else [])\n",
    "# for model_name, model_path in name_model_paths.items():\n",
    "#     clear_cuda_memory()\n",
    "#     benchmark_model(name_dataset = name_dataset,\n",
    "#                     model_path = model_path,\n",
    "#                     prompt_template = prompt_template,\n",
    "#                     input_df = medication_status_test,\n",
    "#                     data_folder = data_folder,\n",
    "#                     result_df_path = results_df_path,\n",
    "#                     use_sampling = False,\n",
    "#                     batch_size = 16,\n",
    "#                     max_token_output = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Accuracy_avg</th>\n",
       "      <th>Precision_avg</th>\n",
       "      <th>Recall_avg</th>\n",
       "      <th>F1_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MIMIC-IV</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MIMIC-IV</td>\n",
       "      <td>Qwen2.5-14B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MIMIC-IV</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MIMIC-IV</td>\n",
       "      <td>Llama-3.1-8B</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MIMIC-IV</td>\n",
       "      <td>Qwen2-7B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MIMIC-IV</td>\n",
       "      <td>Llama-3.2-1B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MIMIC-IV</td>\n",
       "      <td>Bio_ClinicalBERT</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.367</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MIMIC-IV</td>\n",
       "      <td>meditron-7b</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MIT</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MIT</td>\n",
       "      <td>Qwen2.5-14B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MIT</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MIT</td>\n",
       "      <td>Llama-3.1-8B</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MIT</td>\n",
       "      <td>Qwen2-7B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIT</td>\n",
       "      <td>Llama-3.2-1B-Instruct</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MIT</td>\n",
       "      <td>Bio_ClinicalBERT</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.367</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MIT</td>\n",
       "      <td>meditron-7b</td>\n",
       "      <td>\\nIdentify and categorize the medications ment...</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset                  Model  \\\n",
       "10  MIMIC-IV  Llama-3.1-8B-Instruct   \n",
       "14  MIMIC-IV   Qwen2.5-14B-Instruct   \n",
       "12  MIMIC-IV  Llama-3.2-3B-Instruct   \n",
       "9   MIMIC-IV           Llama-3.1-8B   \n",
       "13  MIMIC-IV      Qwen2-7B-Instruct   \n",
       "11  MIMIC-IV  Llama-3.2-1B-Instruct   \n",
       "8   MIMIC-IV       Bio_ClinicalBERT   \n",
       "15  MIMIC-IV            meditron-7b   \n",
       "2        MIT  Llama-3.1-8B-Instruct   \n",
       "6        MIT   Qwen2.5-14B-Instruct   \n",
       "4        MIT  Llama-3.2-3B-Instruct   \n",
       "1        MIT           Llama-3.1-8B   \n",
       "5        MIT      Qwen2-7B-Instruct   \n",
       "3        MIT  Llama-3.2-1B-Instruct   \n",
       "0        MIT       Bio_ClinicalBERT   \n",
       "7        MIT            meditron-7b   \n",
       "\n",
       "                                               Prompt  Accuracy_avg  \\\n",
       "10  \\nIdentify and categorize the medications ment...         0.649   \n",
       "14  \\nIdentify and categorize the medications ment...         0.611   \n",
       "12  \\nIdentify and categorize the medications ment...         0.589   \n",
       "9   \\nIdentify and categorize the medications ment...         0.414   \n",
       "13  \\nIdentify and categorize the medications ment...         0.384   \n",
       "11  \\nIdentify and categorize the medications ment...         0.377   \n",
       "8   \\nIdentify and categorize the medications ment...         0.367   \n",
       "15  \\nIdentify and categorize the medications ment...         0.038   \n",
       "2   \\nIdentify and categorize the medications ment...         0.679   \n",
       "6   \\nIdentify and categorize the medications ment...         0.601   \n",
       "4   \\nIdentify and categorize the medications ment...         0.577   \n",
       "1   \\nIdentify and categorize the medications ment...         0.427   \n",
       "5   \\nIdentify and categorize the medications ment...         0.392   \n",
       "3   \\nIdentify and categorize the medications ment...         0.381   \n",
       "0   \\nIdentify and categorize the medications ment...         0.367   \n",
       "7   \\nIdentify and categorize the medications ment...         0.050   \n",
       "\n",
       "    Precision_avg  Recall_avg  F1_avg  \n",
       "10          0.735       0.775   0.675  \n",
       "14          0.716       0.756   0.636  \n",
       "12          0.766       0.719   0.625  \n",
       "9           0.530       0.581   0.433  \n",
       "13          0.453       0.629   0.402  \n",
       "11          0.762       0.487   0.390  \n",
       "8           1.000       0.367   0.367  \n",
       "15          0.061       0.381   0.042  \n",
       "2           0.767       0.790   0.704  \n",
       "6           0.706       0.751   0.626  \n",
       "4           0.758       0.721   0.615  \n",
       "1           0.541       0.600   0.445  \n",
       "5           0.462       0.629   0.409  \n",
       "3           0.764       0.492   0.395  \n",
       "0           1.000       0.367   0.367  \n",
       "7           0.074       0.386   0.052  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.read_csv(results_df_path).sort_values(by=['Dataset', 'F1'], ascending=[True, False])\n",
    "result_df.columns = ['Dataset', 'Model', 'Prompt', 'Accuracy_avg', 'Precision_avg', 'Recall_avg', 'F1_avg']\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
